{
  "labels": {
    "languageLabel": "Select Language",
    "aboutTitle": "About Me",
    "aboutSubtitle": "Applied Machine Learning · Analytics Systems",
    "aboutDescription": "Applied Data Scientist and Mechatronics Engineer with experience building data pipelines, predictive models, and monitoring systems for high-reliability, data-intensive environments across industrial and financial domains.\n\nI focus on designing scalable analytics solutions that improve operational efficiency, cost optimization, risk monitoring, and decision-making, combining strong analytical thinking with practical engineering judgment.",
    "aboutCta": "↓ View selected projects",
    "problemLabel": "Problem",
    "impactLabel": "Impact",
    "projectsTitle": "Projects",
    "projectsSubtitle": "Selected work and measurable impact across mining and banking.",
    "stackTitle": "Tech Stack",
    "stackSubtitle": "Tools and technologies I use in production.",
    "contactTitle": "Contact Me",
    "contactDescription": "Feel free to reach out to me through any of the platforms below or via email.",
    "navAbout": "About Me",
    "navProjects": "Projects",
    "navStack": "Tech Stack",
    "navContact": "Contact",
    "backToHome": "Back to home",
    "projectDetailsTitle": "Project Details",
    "highlightsLabel": "Highlights"
  },
  "projects": [
    {
      "slug": "ml-systems/cas-monitoring-system",
      "title": "Anti-collision Device Health Monitoring System",
      "domain": "Mining / Fleet Ops",
      "problem": "No centralized view of CAS device health across fleets.",
      "impact": "Enabled proactive maintenance and critical asset prioritization.",
      "highlights": [
        "Designed a data pipeline and health scoring model for CAS devices, improving fleet visibility and maintenance decisions."
      ],
      "tags": [
        "SQL Server",
        "MongoDB",
        "Power BI",
        "Health Scoring"
      ],
      "i18n": {
        "cas": {
          "hero": {
            "eyebrow": "Device Health Monitoring Platform",
            "title": "CAS Device Health Monitoring Platform",
            "subtitle": "Proactive monitoring and health scoring for collision-avoidance devices across industrial fleets.",
            "chips": [
              "SQL Server",
              "MongoDB",
              "Power BI",
              "Health Scoring"
            ],
            "confidentialNote": "Screenshots and diagrams are anonymized due to NDA."
          },
          "panel": {
            "focus": {
              "label": "Focus",
              "value": "Fleet-wide visibility"
            },
            "outcome": {
              "label": "Outcome",
              "value": "Proactive maintenance decisions"
            },
            "audience": {
              "label": "Audience",
              "value": "Field engineers and support teams"
            },
            "scope": {
              "label": "Deployment Scope",
              "countries": [
                "Peru",
                "Brazil",
                "Chile",
                "Colombia",
                "Mexico"
              ]
            }
          },
          "summary": {
            "title": "Project Summary",
            "subtitle": "Quick scan of the core story.",
            "cards": [
              {
                "title": "Problem",
                "body": "Lack of centralized monitoring to understand CAS device health across fleets."
              },
              {
                "title": "Solution",
                "body": "Data pipelines with health scoring to unify status, connectivity, and history."
              },
              {
                "title": "Impact",
                "body": "Proactive maintenance planning and prioritization of critical devices."
              },
              {
                "title": "Users",
                "body": "Support engineers and technical support teams."
              }
            ]
          },
          "dashboard": {
            "title": "Key Dashboard Views",
            "subtitle": "Two key views that drive daily decisions.",
            "views": [
              {
                "alt": "CAS dashboard overview with connectivity and device health trends",
                "title": "Connectivity and Fleet Health",
                "bullets": [
                  "Highlights drop-off trends and devices at risk.",
                  "Tracks historical indicators to spot recurring issues."
                ]
              },
              {
                "alt": "CAS dashboard with per-device variables and daily scores",
                "title": "Daily Health Scoring",
                "bullets": [
                  "Ranks assets by current criticality.",
                  "Guides maintenance scheduling and field checks."
                ]
              }
            ]
          },
          "architecture": {
            "title": "Data Pipeline Architecture",
            "subtitle": "From device telemetry to analytics.",
            "alt": "Architecture diagram for CAS data and processing flow",
            "items": [
              "Data sources: MongoDB and SQL Server.",
              "Scheduled pipelines for daily ingestion and scoring.",
              "Historical storage for trend analysis.",
              "Analytics layer powered by Power BI."
            ]
          },
          "highlights": {
            "title": "Key Technical Highlights",
            "subtitle": "Engineering choices that shaped the solution.",
            "cards": [
              {
                "title": "Weighted health scoring",
                "body": "70% hardware signals and 30% connectivity indicators for balanced prioritization."
              },
              {
                "title": "Daily historization",
                "body": "Time-series storage to compare current status against long-term behavior."
              },
              {
                "title": "Critical asset ranking",
                "body": "Automatic prioritization for field teams and maintenance planning."
              },
              {
                "title": "Maintenance-first design",
                "body": "Focused on reducing downtime and guiding preventive actions."
              }
            ]
          },
          "role": {
            "title": "Role & Responsibilities",
            "subtitle": "Ownership across data, analytics, and stakeholder support.",
            "items": [
              "Designed the end-to-end data pipeline.",
              "Modeled and optimized SQL Server storage.",
              "Implemented historization and data quality logic.",
              "Defined technical metrics and health scoring.",
              "Built analytical views for Power BI dashboards.",
              "Supported engineering and maintenance users."
            ]
          },
          "stack": {
            "title": "Tech Stack",
            "subtitle": "Grouped by the layers used in production.",
            "groups": [
              {
                "title": "Data & Storage",
                "items": [
                  "SQL Server",
                  "MongoDB"
                ]
              },
              {
                "title": "Processing",
                "items": [
                  "Stored Procedures",
                  "Scheduled Jobs"
                ]
              },
              {
                "title": "Analytics & Visualization",
                "items": [
                  "Power BI"
                ]
              }
            ]
          },
          "lessons": {
            "summary": "Lessons learned",
            "items": [
              "Effective monitoring needs historical context, not just current status.",
              "Data quality is a first-class metric, not a byproduct.",
              "Weighted scoring accelerates operational decision-making.",
              "Reducing noise is as important as detecting failures."
            ]
          }
        }
      }
    },
    {
      "slug": "risk-modeling/over-indebtedness-model",
      "title": "Over-Indebtedness Detection Model",
      "domain": "Banking / Risk",
      "problem": "Detect over-indebted leads with higher separation.",
      "impact": "83% precision and +11% separation between good and bad leads.",
      "highlights": [
        "Developed a model to detect over-indebtedness with 83% accuracy and an 11% separation between good and bad leads (up from 5%)."
      ],
      "tags": [
        "Python",
        "Scikit-learn",
        "Risk"
      ]
    },
    {
      "slug": "anomaly-detection/data-quality-monitoring",
      "title": "Data Latency & Reporting Feasibility Analysis",
      "domain": "Mining / Data Quality",
      "problem": "Unclear data arrival timing across sources when defining reporting frequency.",
      "impact": "Enabled realistic reporting cadence and informed targeted improvements when needed.",
      "highlights": [
        "Assessed data latency across MongoDB, SQL Server alarm data, and GPS sources to define feasible near real-time reporting.",
        "Supported operational and commercial decisions by identifying optimal reporting windows per client.",
        "Enabled follow-up improvements and validated progress through repeated, on-demand analysis."
      ],
      "tags": [
        "Python",
        "MongoDB",
        "SQL Server",
        "Power BI",
        "Data Quality",
        "Latency Analysis"
      ],
      "i18n": {
        "dataQuality": {
          "hero": {
            "eyebrow": "Data Latency Assessment",
            "title": "Data Latency & Reporting Feasibility Analysis",
            "subtitle": "On-demand analysis to evaluate data arrival timing and define feasible reporting frequency across clients.",
            "chips": [
              "Python",
              "MongoDB",
              "SQL Server",
              "Power BI"
            ],
            "confidentialNote": "Screenshots and diagrams are anonymized due to NDA."
          },
          "panel": {
            "focus": {
              "label": "Focus",
              "value": "Data arrival latency and reporting feasibility"
            },
            "outcome": {
              "label": "Outcome",
              "value": "Defined optimal reporting cadence per client"
            },
            "audience": {
              "label": "Audience",
              "value": "Remote support teams and contract administrators"
            },
            "scope": {
              "label": "Deployment Scope",
              "countries": [
                "Peru",
                "Brazil",
                "Chile",
                "Colombia",
                "Mexico"
              ]
            }
          },
          "summary": {
            "title": "Project Summary",
            "subtitle": "Why and how reporting cadence was defined.",
            "cards": [
              {
                "title": "Problem",
                "body": "Reporting frequency needed to be aligned with actual data arrival behavior across systems."
              },
              {
                "title": "Solution",
                "body": "Performed on-demand latency analysis across MongoDB, complementary SQL Server alarm tables, and GPS data."
              },
              {
                "title": "Impact",
                "body": "Provided a clear reference for near real-time reporting and guidance on when improvements were required."
              },
              {
                "title": "Users",
                "body": "Remote support engineers and contract administrators responsible for operational monitoring."
              }
            ]
          },
          "dashboard": {
            "title": "Key Dashboard Views",
            "subtitle": "Visual tools used to assess reporting feasibility.",
            "views": [
              {
                "alt": "Power BI overview showing latency distribution by data source with color-coded criticality",
                "title": "Latency Overview by Source",
                "bullets": [
                  "Aggregated latency by data source to quickly understand typical arrival times.",
                  "Color scale indicating acceptable, moderate, and high delay ranges.",
                  "Used to define a realistic reporting window without manual calculations."
                ]
              },
              {
                "alt": "Power BI detail view showing per-vehicle average daily latency",
                "title": "Per-Asset Latency Detail",
                "bullets": [
                  "Drill-down to vehicle-level latency for deeper operational insight.",
                  "Helps distinguish systemic behavior from isolated cases.",
                  "Useful for validating improvements after network or process changes."
                ]
              }
            ]
          },
          "architecture": {
            "title": "Data Analysis Architecture",
            "subtitle": "Sources and processing used for latency assessment.",
            "alt": "Architecture diagram for latency analysis using MongoDB, SQL Server, GPS data, Python processing, and Power BI",
            "items": [
              "Data sources included MongoDB, SQL Server alarm data (complementary tables), and GPS records.",
              "Processing in Python and SQL Server for calculating latency metrics.",
              "Results exported as CSV files for flexible, client-specific analysis.",
              "Power BI dashboards loaded directly from CSV folders for rapid updates."
            ]
          },
          "highlights": {
            "title": "Key Technical Highlights",
            "subtitle": "What made the analysis practical and reusable.",
            "cards": [
              {
                "title": "On-demand assessment",
                "body": "Designed to be executed once or repeated as needed, rather than continuous monitoring."
              },
              {
                "title": "Latency bucketing",
                "body": "Grouped delays into intuitive ranges (minutes to days) to support decision-making."
              },
              {
                "title": "Multi-source coverage",
                "body": "Considered MongoDB, SQL Server alarm data, and GPS sources for a complete view."
              },
              {
                "title": "Operational enablement",
                "body": "Dashboard visuals allowed non-technical users to interpret results quickly."
              }
            ]
          },
          "role": {
            "title": "Role & Responsibilities",
            "subtitle": "Collaboration across analysis, tooling, and enablement.",
            "items": [
              "Defined the methodology to assess data latency and reporting feasibility.",
              "Collaborated with remote support teams to extract and validate data.",
              "Implemented Python processing and Power BI dashboards.",
              "Trained remote support personnel to perform future analyses independently.",
              "Supported contract administrators with data-driven reporting guidance."
            ]
          },
          "stack": {
            "title": "Tech Stack",
            "subtitle": "Tools used across the workflow.",
            "groups": [
              {
                "title": "Data Sources",
                "items": [
                  "MongoDB",
                  "SQL Server",
                  "GPS Data"
                ]
              },
              {
                "title": "Processing",
                "items": [
                  "Python",
                  "CSV"
                ]
              },
              {
                "title": "Visualization",
                "items": [
                  "Power BI"
                ]
              }
            ]
          },
          "lessons": {
            "summary": "Lessons learned",
            "items": [
              "Reporting frequency should reflect observed data arrival, not assumptions.",
              "A single, well-executed analysis is often sufficient to define reporting cadence.",
              "Clear visuals help align technical findings with operational and contractual decisions.",
              "Knowledge transfer ensures sustainability beyond the initial analysis."
            ]
          }
        }
      }
    },
    {
      "slug": "anomaly-detection/sensor-failure-gps",
      "title": "Reverse Sensor Failure Detection",
      "domain": "Mining / Telematics",
      "problem": "Detect reverse sensor failures before downtime.",
      "impact": "90% precision, enabled proactive maintenance.",
      "highlights": [
        "Implemented an algorithm to detect reverse sensor failures with 90% accuracy using GPS data and kinematic models."
      ],
      "tags": [
        "Python",
        "Pandas",
        "SQL"
      ]
    },
    {
      "slug": "graph-ml/mine-sectorization",
      "title": "Mine Sectorization with Graph Clustering",
      "domain": "Mining / Graph ML",
      "problem": "Sectorize the mine into strategic monitoring areas.",
      "impact": "Structured monitoring zones using clustering and label propagation.",
      "highlights": [
        "Developed a model to sectorize the mine into strategic areas using clustering, graph theory, and label propagation."
      ],
      "tags": [
        "Python",
        "Graph Theory",
        "Clustering"
      ]
    },
    {
      "slug": "dashboards/operator-behavior-monitor",
      "title": "Operator Alert Monitoring and Driving Score",
      "domain": "Safety / Operations",
      "problem": "Score operator driving to take preventive action.",
      "impact": "Preventive measures guided by operator alert scoring.",
      "highlights": [
        "Built an operator alert monitoring tool that scores driving to support preventive measures."
      ],
      "tags": [
        "Python",
        "Dashboards",
        "Safety"
      ]
    },
    {
      "slug": "dashboards/network-signal-kpi",
      "title": "Network Signal Quality Monitoring",
      "domain": "Network / Monitoring",
      "problem": "Monitor KPIs for field coverage and signal quality.",
      "impact": "Automated ETL and dashboards for network KPIs.",
      "highlights": [
        "Developed an automated ETL and KPI dashboard to monitor field coverage and signal quality."
      ],
      "tags": [
        "Python",
        "ETL",
        "Dashboards"
      ]
    },
    
    {
      "slug": "graph-ml/income-inference-graph",
      "title": "Income Inference with Graph Segmentation",
      "domain": "Banking / Graph ML",
      "problem": "Improve income inference accuracy with graph segmentation.",
      "impact": "+2% accuracy gain on income inference.",
      "highlights": [
        "Complemented the income inference model with a graph-based segmenter, improving accuracy by +2%."
      ],
      "tags": [
        "Python",
        "Graph Theory",
        "ML"
      ]
    },
    {
      "slug": "ml-systems/nrt-alerting-pipeline",
      "title": "ML Controls Monitoring Reports",
      "domain": "Banking / Model Ops",
      "problem": "Detect incidents in monthly ML control execution.",
      "impact": "20+ summarized reports to catch stability and score issues.",
      "highlights": [
        "Delivered 20+ summarized reports to detect incidents in monthly execution controls for retail risk and collections ML models."
      ],
      "tags": [
        "Python",
        "Reporting",
        "Model Monitoring"
      ]
    }
  ]
}