{
  "labels": {
    "languageLabel": "Select Language",
    "aboutTitle": "About Me",
    "aboutSubtitle": "Applied Machine Learning · Analytics Systems",
    "aboutDescription": "Applied Data Scientist and Mechatronics Engineer with experience building data pipelines, predictive models, and monitoring systems for high-reliability, data-intensive environments across industrial and financial domains.\n\nI focus on designing scalable analytics solutions that improve operational efficiency, cost optimization, risk monitoring, and decision-making, combining strong analytical thinking with practical engineering judgment.",
    "aboutCta": "↓ View selected projects",
    "problemLabel": "Problem",
    "impactLabel": "Impact",
    "projectsTitle": "Projects",
    "projectsSubtitle": "Selected work and measurable impact across mining and banking.",
    "stackTitle": "Tech Stack",
    "stackSubtitle": "Tools and technologies I use in production.",
    "contactTitle": "Contact Me",
    "contactDescription": "Feel free to reach out to me through any of the platforms below or via email.",
    "navAbout": "About Me",
    "navProjects": "Projects",
    "navStack": "Tech Stack",
    "navContact": "Contact",
    "backToHome": "Back to home",
    "projectDetailsTitle": "Project Details",
    "highlightsLabel": "Highlights"
  },
  "projects": [
    {
      "slug": "ml-systems/cas-monitoring-system",
      "title": "Anti-collision Device Health Monitoring System",
      "domain": "Mining / Fleet Ops",
      "problem": "No centralized view of CAS device health across fleets.",
      "impact": "Enabled proactive maintenance and critical asset prioritization.",
      "highlights": [
        "Designed a data pipeline and health scoring model for CAS devices, improving fleet visibility and maintenance decisions."
      ],
      "tags": [
        "SQL Server",
        "MongoDB",
        "Power BI",
        "Health Scoring"
      ],
      "i18n": {
        "cas": {
          "hero": {
            "eyebrow": "Device Health Monitoring Platform",
            "title": "CAS Device Health Monitoring Platform",
            "subtitle": "Proactive monitoring and health scoring for collision-avoidance devices across industrial fleets.",
            "chips": [
              "SQL Server",
              "MongoDB",
              "Power BI",
              "Health Scoring"
            ],
            "confidentialNote": "Screenshots and diagrams are anonymized due to NDA."
          },
          "panel": {
            "focus": {
              "label": "Focus",
              "value": "Fleet-wide visibility"
            },
            "outcome": {
              "label": "Outcome",
              "value": "Proactive maintenance decisions"
            },
            "audience": {
              "label": "Audience",
              "value": "Field engineers and support teams"
            },
            "scope": {
              "label": "Deployment Scope",
              "countries": [
                "Peru",
                "Brazil",
                "Chile",
                "Colombia",
                "Mexico"
              ]
            }
          },
          "summary": {
            "title": "Project Summary",
            "subtitle": "Quick scan of the core story.",
            "cards": [
              {
                "title": "Problem",
                "body": "Lack of centralized monitoring to understand CAS device health across fleets."
              },
              {
                "title": "Solution",
                "body": "Data pipelines with health scoring to unify status, connectivity, and history."
              },
              {
                "title": "Impact",
                "body": "Proactive maintenance planning and prioritization of critical devices."
              },
              {
                "title": "Users",
                "body": "Support engineers and technical support teams."
              }
            ]
          },
          "dashboard": {
            "title": "Key Dashboard Views",
            "subtitle": "Two key views that drive daily decisions.",
            "views": [
              {
                "alt": "CAS dashboard overview with connectivity and device health trends",
                "title": "Connectivity and Fleet Health",
                "bullets": [
                  "Highlights drop-off trends and devices at risk.",
                  "Tracks historical indicators to spot recurring issues."
                ]
              },
              {
                "alt": "CAS dashboard with per-device variables and daily scores",
                "title": "Daily Health Scoring",
                "bullets": [
                  "Ranks assets by current criticality.",
                  "Guides maintenance scheduling and field checks."
                ]
              }
            ]
          },
          "architecture": {
            "title": "Data Pipeline Architecture",
            "subtitle": "From device telemetry to analytics.",
            "alt": "Architecture diagram for CAS data and processing flow",
            "items": [
              "Data sources: MongoDB and SQL Server.",
              "Scheduled pipelines for daily ingestion and scoring.",
              "Historical storage for trend analysis.",
              "Analytics layer powered by Power BI."
            ]
          },
          "highlights": {
            "title": "Key Technical Highlights",
            "subtitle": "Engineering choices that shaped the solution.",
            "cards": [
              {
                "title": "Weighted health scoring",
                "body": "70% hardware signals and 30% connectivity indicators for balanced prioritization."
              },
              {
                "title": "Daily historization",
                "body": "Time-series storage to compare current status against long-term behavior."
              },
              {
                "title": "Critical asset ranking",
                "body": "Automatic prioritization for field teams and maintenance planning."
              },
              {
                "title": "Maintenance-first design",
                "body": "Focused on reducing downtime and guiding preventive actions."
              }
            ]
          },
          "role": {
            "title": "Role & Responsibilities",
            "subtitle": "Ownership across data, analytics, and stakeholder support.",
            "items": [
              "Designed the end-to-end data pipeline.",
              "Modeled and optimized SQL Server storage.",
              "Implemented historization and data quality logic.",
              "Defined technical metrics and health scoring.",
              "Built analytical views for Power BI dashboards.",
              "Supported engineering and maintenance users."
            ]
          },
          "stack": {
            "title": "Tech Stack",
            "subtitle": "Grouped by the layers used in production.",
            "groups": [
              {
                "title": "Data & Storage",
                "items": [
                  "SQL Server",
                  "MongoDB"
                ]
              },
              {
                "title": "Processing",
                "items": [
                  "Stored Procedures",
                  "Scheduled Jobs"
                ]
              },
              {
                "title": "Analytics & Visualization",
                "items": [
                  "Power BI"
                ]
              }
            ]
          },
          "lessons": {
            "summary": "Lessons learned",
            "items": [
              "Effective monitoring needs historical context, not just current status.",
              "Data quality is a first-class metric, not a byproduct.",
              "Weighted scoring accelerates operational decision-making.",
              "Reducing noise is as important as detecting failures."
            ]
          }
        }
      }
    },
    {
      "slug": "anomaly-detection/sensor-failure-gps",
      "title": "Reverse Sensor Failure Detection",
      "domain": "Mining / Telematics",
      "problem": "Detect reverse sensor failures before downtime.",
      "impact": "90% precision, enabled proactive maintenance.",
      "highlights": [
        "Implemented an algorithm to detect reverse sensor failures with 90% accuracy using GPS data and kinematic models."
      ],
      "tags": [
        "Python",
        "Pandas",
        "SQL"
      ]
    },
    {
      "slug": "anomaly-detection/data-quality-monitoring",
      "title": "Near Real-Time Data Quality Monitoring (Data Latency Analysis)",
      "domain": "Mining / Data Quality",
      "problem": "Unclear end-to-end data latency across sources, making near real-time alarm reporting unreliable and hard to scope for each client.",
      "impact": "Defined realistic reporting frequency per client, identified bottlenecks, and enabled measurable network/data-flow improvements (e.g., reduced extreme delays from ~5 days to ~1 day).",
      "highlights": [
        "Analyzed data latency (minutes to days) across MongoDB and SQL Server sources to determine the optimal, reliable reporting window.",
        "Identified bottlenecks per source (alarm header vs alarm details in SQL Server) and supported client-side improvements, validating progress over time with monitoring.",
        "Built a Power BI dashboard to classify latency criticality (green/yellow/red) and drill down from source-level summary to per-vehicle daily latency."
      ],
      "tags": [
        "Python",
        "MongoDB",
        "SQL Server",
        "Power BI",
        "Monitoring"
      ],
      "i18n": {
        "dataQuality": {
          "hero": {
            "eyebrow": "Data Latency & Quality Monitoring",
            "title": "Near Real-Time Data Quality Monitoring",
            "subtitle": "Latency analysis across MongoDB and SQL Server to define reliable reporting frequency, detect bottlenecks, and guide network/data-flow improvements.",
            "chips": [
              "Python",
              "MongoDB",
              "SQL Server",
              "Power BI",
              "Monitoring"
            ],
            "confidentialNote": "Screenshots and diagrams are anonymized due to NDA."
          },
          "panel": {
            "focus": {
              "label": "Focus",
              "value": "Data latency (minutes → days) and bottleneck detection"
            },
            "outcome": {
              "label": "Outcome",
              "value": "Reliable reporting window per client + actionable improvements"
            },
            "audience": {
              "label": "Audience",
              "value": "Sales, remote support engineers, and stakeholders defining reporting SLAs"
            },
            "scope": {
              "label": "Deployment Scope",
              "countries": [
                "Peru",
                "Brazil",
                "Chile",
                "Colombia",
                "Mexico"
              ]
            }
          },
          "summary": {
            "title": "Project Summary",
            "subtitle": "Quick scan of the core story.",
            "cards": [
              {
                "title": "Problem",
                "body": "Reporting strategies were being proposed without knowing the true end-to-end delay of data arrival, risking unrealistic near real-time promises."
              },
              {
                "title": "Solution",
                "body": "Measured latency across MongoDB and SQL Server (alarm table + final alarm details table), categorized delays, and pinpointed bottlenecks by source and client."
              },
              {
                "title": "Impact",
                "body": "Enabled realistic reporting frequency per client (2 minutes to multiple hours/days). Supported client conversations to improve networks/data flow and validated improvements (e.g., ~5 days → ~1 day)."
              },
              {
                "title": "Users",
                "body": "Sales teams to define feasible offerings, and remote support engineers trained to operate and interpret the monitoring dashboard."
              }
            ]
          },
          "dashboard": {
            "title": "Key Dashboard Views",
            "subtitle": "Two views used to define reporting frequency and identify bottlenecks.",
            "views": [
              {
                "alt": "Power BI summary view showing data latency by source with criticality colors and distribution buckets",
                "title": "Latency Summary by Data Source",
                "bullets": [
                  "Aggregates latency per source to quickly identify bottlenecks.",
                  "Criticality buckets (e.g., 2 min, 30 min, 1 h, 4 h, 1 day, >5 days) to define feasible reporting cadence.",
                  "Includes KPI for % of data arriving within 30 minutes (target threshold ~80% for acceptable near real-time reporting).",
                  "Color scale: green (≤30 min acceptable), yellow (medium delay), red (high delay)."
                ]
              },
              {
                "alt": "Power BI detail view showing per-vehicle daily average latency and drill-down analysis",
                "title": "Per-Vehicle Daily Latency Detail",
                "bullets": [
                  "Drill-down to individual vehicles to spot client-specific and asset-specific delay patterns.",
                  "Daily average latency tracking to validate improvements over time after network/data-flow changes.",
                  "Helps distinguish systemic source bottlenecks vs isolated connectivity cases."
                ]
              }
            ]
          },
          "architecture": {
            "title": "Data Pipeline Architecture",
            "subtitle": "From operational data sources to latency analysis and monitoring.",
            "alt": "Architecture diagram for data latency monitoring pipeline using MongoDB, SQL Server, Python processing, CSV outputs, and Power BI dashboards",
            "items": [
              "Data sources: MongoDB and SQL Server.",
              "SQL Server sources included both the alarm events table and the final alarm details table to measure end-to-end delay.",
              "Python ETL to extract and compute latency metrics (minutes to days), generate per-client outputs.",
              "Outputs exported as CSV files and loaded into Power BI.",
              "Power BI refresh driven by loading a client CSV folder, enabling fast switching and updates."
            ]
          },
          "highlights": {
            "title": "Key Technical Highlights",
            "subtitle": "Engineering choices that made the analysis actionable across clients.",
            "cards": [
              {
                "title": "End-to-end latency measurement",
                "body": "Computed time deltas in minutes across multiple hops (MongoDB → SQL Server) and across SQL Server stages (alarm record vs final alarm details)."
              },
              {
                "title": "Criticality buckets + SLA guidance",
                "body": "Mapped latency into decision buckets (2 min, 30 min, 1 h, 4 h, 1 day, >5 days) to recommend realistic reporting frequency per client."
              },
              {
                "title": "Source bottleneck isolation",
                "body": "Separated delays by data source and table stage to identify whether the bottleneck was upstream ingestion, processing, or network constraints."
              },
              {
                "title": "Operational dashboarding",
                "body": "Power BI views with color-based severity and drill-down (source → vehicle) enabled support and sales to act without manual recalculation."
              }
            ]
          },
          "role": {
            "title": "Role & Responsibilities",
            "subtitle": "Ownership across analysis, pipeline, dashboarding, and enablement.",
            "items": [
              "Defined the objective and methodology to determine a reliable reporting window based on observed latency distributions.",
              "Explored and measured delays across MongoDB and SQL Server sources, including alarm and alarm-details tables.",
              "Built Python extraction and processing logic to compute latency metrics and generate client-ready datasets.",
              "Designed and implemented Power BI dashboards (summary + per-vehicle drill-down) with criticality coloring.",
              "Supported stakeholder discussions (sales + client coordination) to request and justify network/data-flow improvements.",
              "Trained remote support engineers to use and interpret the monitoring tool."
            ]
          },
          "stack": {
            "title": "Tech Stack",
            "subtitle": "Grouped by the layers used in production.",
            "groups": [
              {
                "title": "Data & Storage",
                "items": [
                  "MongoDB",
                  "SQL Server"
                ]
              },
              {
                "title": "Processing / ETL",
                "items": [
                  "Python",
                  "CSV Exports"
                ]
              },
              {
                "title": "Analytics & Visualization",
                "items": [
                  "Power BI"
                ]
              }
            ]
          },
          "lessons": {
            "summary": "Lessons learned",
            "items": [
              "Near real-time reporting must be grounded in observed data arrival distributions, not assumptions.",
              "Separating latency by source and processing stage makes bottlenecks actionable (not just visible).",
              "Simple thresholds (e.g., % within 30 minutes) accelerate decisions for SLAs and go-to-market expectations.",
              "Monitoring enables proof of improvement after client-side changes, strengthening collaboration and trust."
            ]
          }
        }
      }
    },
    {
      "slug": "graph-ml/mine-sectorization",
      "title": "Mine Sectorization with Graph Clustering",
      "domain": "Mining / Graph ML",
      "problem": "Sectorize the mine into strategic monitoring areas.",
      "impact": "Structured monitoring zones using clustering and label propagation.",
      "highlights": [
        "Developed a model to sectorize the mine into strategic areas using clustering, graph theory, and label propagation."
      ],
      "tags": [
        "Python",
        "Graph Theory",
        "Clustering"
      ]
    },
    {
      "slug": "dashboards/operator-behavior-monitor",
      "title": "Operator Alert Monitoring and Driving Score",
      "domain": "Safety / Operations",
      "problem": "Score operator driving to take preventive action.",
      "impact": "Preventive measures guided by operator alert scoring.",
      "highlights": [
        "Built an operator alert monitoring tool that scores driving to support preventive measures."
      ],
      "tags": [
        "Python",
        "Dashboards",
        "Safety"
      ]
    },
    {
      "slug": "dashboards/network-signal-kpi",
      "title": "Network Signal Quality Monitoring",
      "domain": "Network / Monitoring",
      "problem": "Monitor KPIs for field coverage and signal quality.",
      "impact": "Automated ETL and dashboards for network KPIs.",
      "highlights": [
        "Developed an automated ETL and KPI dashboard to monitor field coverage and signal quality."
      ],
      "tags": [
        "Python",
        "ETL",
        "Dashboards"
      ]
    },
    {
      "slug": "risk-modeling/over-indebtedness-model",
      "title": "Over-Indebtedness Detection Model",
      "domain": "Banking / Risk",
      "problem": "Detect over-indebted leads with higher separation.",
      "impact": "83% precision and +11% separation between good and bad leads.",
      "highlights": [
        "Developed a model to detect over-indebtedness with 83% accuracy and an 11% separation between good and bad leads (up from 5%)."
      ],
      "tags": [
        "Python",
        "Scikit-learn",
        "Risk"
      ]
    },
    {
      "slug": "graph-ml/income-inference-graph",
      "title": "Income Inference with Graph Segmentation",
      "domain": "Banking / Graph ML",
      "problem": "Improve income inference accuracy with graph segmentation.",
      "impact": "+2% accuracy gain on income inference.",
      "highlights": [
        "Complemented the income inference model with a graph-based segmenter, improving accuracy by +2%."
      ],
      "tags": [
        "Python",
        "Graph Theory",
        "ML"
      ]
    },
    {
      "slug": "ml-systems/nrt-alerting-pipeline",
      "title": "ML Controls Monitoring Reports",
      "domain": "Banking / Model Ops",
      "problem": "Detect incidents in monthly ML control execution.",
      "impact": "20+ summarized reports to catch stability and score issues.",
      "highlights": [
        "Delivered 20+ summarized reports to detect incidents in monthly execution controls for retail risk and collections ML models."
      ],
      "tags": [
        "Python",
        "Reporting",
        "Model Monitoring"
      ]
    }
  ]
}