{
  "labels": {
    "languageLabel": "Select Language",
    "aboutTitle": "About Me",
    "aboutSubtitle": "Applied Machine Learning · Analytics Systems",
    "aboutDescription": "Applied Data Scientist and Mechatronics Engineer with experience building data pipelines, predictive models, and monitoring systems for high-reliability, data-intensive environments across industrial and financial domains.\n\nI focus on designing scalable analytics solutions that improve operational efficiency, cost optimization, risk monitoring, and decision-making, combining strong analytical thinking with practical engineering judgment.",
    "aboutCta": "↓ View selected projects",
    "problemLabel": "Problem",
    "impactLabel": "Impact",
    "projectsTitle": "Projects",
    "projectsSubtitle": "Selected work and measurable impact across mining and banking.",
    "stackTitle": "Tech Stack",
    "stackSubtitle": "Tools and technologies I use in production.",
    "contactTitle": "Contact Me",
    "contactDescription": "Feel free to reach out to me through any of the platforms below or via email.",
    "navAbout": "About Me",
    "navProjects": "Projects",
    "navStack": "Tech Stack",
    "navContact": "Contact",
    "backToHome": "Back to home",
    "projectDetailsTitle": "Project Details",
    "highlightsLabel": "Highlights"
  },
  "projects": [
    {
      "slug": "ml-systems/cas-monitoring-system",
      "title": "Anti-collision Device Health Monitoring System",
      "domain": "Mining / Fleet Ops",
      "problem": "No centralized view of CAS device health across fleets.",
      "impact": "Enabled proactive maintenance and critical asset prioritization.",
      "highlights": [
        "Designed a data pipeline and health scoring model for CAS devices, improving fleet visibility and maintenance decisions."
      ],
      "tags": [
        "SQL Server",
        "MongoDB",
        "Power BI",
        "Health Scoring"
      ],
      "i18n": {
        "cas": {
          "hero": {
            "eyebrow": "Device Health Monitoring Platform",
            "title": "CAS Device Health Monitoring Platform",
            "subtitle": "Proactive monitoring and health scoring for collision-avoidance devices across industrial fleets.",
            "chips": [
              "SQL Server",
              "MongoDB",
              "Power BI",
              "Health Scoring"
            ],
            "confidentialNote": "Screenshots and diagrams are anonymized due to NDA."
          },
          "panel": {
            "focus": {
              "label": "Focus",
              "value": "Fleet-wide visibility"
            },
            "outcome": {
              "label": "Outcome",
              "value": "Proactive maintenance decisions"
            },
            "audience": {
              "label": "Audience",
              "value": "Field engineers and support teams"
            },
            "scope": {
              "label": "Deployment Scope",
              "countries": [
                "Peru",
                "Brazil",
                "Chile",
                "Colombia",
                "Mexico"
              ]
            }
          },
          "summary": {
            "title": "Project Summary",
            "subtitle": "Quick scan of the core story.",
            "cards": [
              {
                "title": "Problem",
                "body": "Lack of centralized monitoring to understand CAS device health across fleets."
              },
              {
                "title": "Solution",
                "body": "Data pipelines with health scoring to unify status, connectivity, and history."
              },
              {
                "title": "Impact",
                "body": "Proactive maintenance planning and prioritization of critical devices."
              },
              {
                "title": "Users",
                "body": "Support engineers and technical support teams."
              }
            ]
          },
          "dashboard": {
            "title": "Key Dashboard Views",
            "subtitle": "Two key views that drive daily decisions.",
            "views": [
              {
                "alt": "CAS dashboard overview with connectivity and device health trends",
                "title": "Connectivity and Fleet Health",
                "bullets": [
                  "Highlights drop-off trends and devices at risk.",
                  "Tracks historical indicators to spot recurring issues."
                ]
              },
              {
                "alt": "CAS dashboard with per-device variables and daily scores",
                "title": "Daily Health Scoring",
                "bullets": [
                  "Ranks assets by current criticality.",
                  "Guides maintenance scheduling and field checks."
                ]
              }
            ]
          },
          "architecture": {
            "title": "Data Pipeline Architecture",
            "subtitle": "From device telemetry to analytics.",
            "alt": "Architecture diagram for CAS data and processing flow",
            "items": [
              "Data sources: MongoDB and SQL Server.",
              "Scheduled pipelines for daily ingestion and scoring.",
              "Historical storage for trend analysis.",
              "Analytics layer powered by Power BI."
            ]
          },
          "highlights": {
            "title": "Key Technical Highlights",
            "subtitle": "Engineering choices that shaped the solution.",
            "cards": [
              {
                "title": "Weighted health scoring",
                "body": "70% hardware signals and 30% connectivity indicators for balanced prioritization."
              },
              {
                "title": "Daily historization",
                "body": "Time-series storage to compare current status against long-term behavior."
              },
              {
                "title": "Critical asset ranking",
                "body": "Automatic prioritization for field teams and maintenance planning."
              },
              {
                "title": "Maintenance-first design",
                "body": "Focused on reducing downtime and guiding preventive actions."
              }
            ]
          },
          "role": {
            "title": "Role & Responsibilities",
            "subtitle": "Ownership across data, analytics, and stakeholder support.",
            "items": [
              "Designed the end-to-end data pipeline.",
              "Modeled and optimized SQL Server storage.",
              "Implemented historization and data quality logic.",
              "Defined technical metrics and health scoring.",
              "Built analytical views for Power BI dashboards.",
              "Supported engineering and maintenance users."
            ]
          },
          "stack": {
            "title": "Tech Stack",
            "subtitle": "Grouped by the layers used in production.",
            "groups": [
              {
                "title": "Data & Storage",
                "items": [
                  "SQL Server",
                  "MongoDB"
                ]
              },
              {
                "title": "Processing",
                "items": [
                  "Stored Procedures",
                  "Scheduled Jobs"
                ]
              },
              {
                "title": "Analytics & Visualization",
                "items": [
                  "Power BI"
                ]
              }
            ]
          },
          "lessons": {
            "summary": "Lessons learned",
            "items": [
              "Effective monitoring needs historical context, not just current status.",
              "Data quality is a first-class metric, not a byproduct.",
              "Weighted scoring accelerates operational decision-making.",
              "Reducing noise is as important as detecting failures."
            ]
          }
        }
      }
    },
    {
      "slug": "risk-modeling/over-indebtedness-model",
      "title": "Over-Indebtedness Detection Model",
      "domain": "Banking / Risk",
      "problem": "Flag over-indebted leads more effectively.",
      "impact": "83% accuracy and +11% separation between good and bad leads.",
      "highlights": [
        "Built a LightGBM classification model to identify over-indebted customers, improving separation between good vs. bad leads from 5% to 11%.",
        "Validated stability across development and out-of-time (OOT) periods and documented the methodology, assumptions, and limitations for governance.",
        "Packaged results and impact tests into stakeholder-ready materials, including ROC/Gini performance and business impact breakdowns."
      ],
      "tags": [
        "Python",
        "LightGBM",
        "SHAP",
        "AWS Athena",
        "AWS S3",
        "AWS SageMaker",
        "Risk Modeling"
      ],
      "i18n": {
        "overIndebtedness": {
          "hero": {
            "eyebrow": "Risk Scoring / Lead Screening",
            "title": "Over-Indebtedness Detection Model",
            "subtitle": "A classification model to strengthen lead screening by identifying over-indebted profiles and improving separation between good and bad outcomes.",
            "chips": [
              "Python",
              "LightGBM",
              "SHAP",
              "AWS Athena",
              "AWS SageMaker"
            ],
            "confidentialNote": "Screenshots, figures, and business metrics are anonymized due to confidentiality."
          },
          "panel": {
            "focus": {
              "label": "Focus",
              "value": "Lead risk screening"
            },
            "outcome": {
              "label": "Outcome",
              "value": "Better separation of risk profiles"
            },
            "audience": {
              "label": "Audience",
              "value": "Risk analysts, credit policy, and business stakeholders"
            },
            "scope": {
              "label": "Deployment Scope",
              "countries": [
                "Peru"
              ]
            }
          },
          "summary": {
            "title": "Project Summary",
            "subtitle": "Quick scan of the core story.",
            "cards": [
              {
                "title": "Problem",
                "body": "Lead screening needed stronger differentiation to detect over-indebted profiles earlier."
              },
              {
                "title": "Solution",
                "body": "A supervised classification model (LightGBM) trained on socio-demographic, financial-system behavior, and internal transactional signals."
              },
              {
                "title": "Impact",
                "body": "Improved separation between good and bad leads to 11% and achieved ~83% overall accuracy."
              },
              {
                "title": "Users",
                "body": "Risk and business teams using the score to support sustainable growth decisions."
              }
            ]
          },
          "dashboard": {
            "title": "Key Stakeholder Views",
            "subtitle": "Artifacts and visuals used to communicate performance and impact.",
            "views": [
              {
                "alt": "Model performance view with ROC/AUC, Gini, confusion matrix and core metrics across train/test/OOT",
                "title": "Performance & Stability",
                "bullets": [
                  "ROC curve and Gini to summarize discrimination power.",
                  "Train/Test/OOT metrics to confirm generalization and stability.",
                  "Confusion matrix to communicate trade-offs and operating point."
                ]
              },
              {
                "alt": "Explainability and impact view with SHAP summary and impact testing matrices/charts",
                "title": "Explainability & Business Impact",
                "bullets": [
                  "SHAP summary to explain key drivers and feature importance.",
                  "Impact tests comparing the new model vs. current approach.",
                  "Business-oriented breakdowns to support adoption decisions."
                ]
              }
            ]
          },
          "architecture": {
            "title": "Pipeline Development Flow",
            "subtitle": "From cloud data to training, evaluation, and stakeholder-ready outputs.",
            "alt": "Flow diagram of the model development pipeline using AWS Athena, S3, SageMaker training workers, evaluation, explainability, and reporting",
            "items": [
              "Ingest & stage data in AWS S3 (multiple internal and external sources).",
              "Feature building and dataset assembly via AWS Athena (SQL) over curated S3 tables.",
              "Create a feature store-style dataset snapshot and export training-ready tables back to S3.",
              "Spin up SageMaker training workers (compute instances) to run Python training pipelines.",
              "Train LightGBM models, run parameter search iterations, and select the final candidate based on performance + stability.",
              "Evaluate on Train/Test and Out-of-Time (OOT) periods; generate ROC/Gini and confusion-matrix metrics.",
              "Compute SHAP explanations and variable importance summaries for model interpretability.",
              "Package outputs (metrics, charts, and impact tests) into stakeholder materials and versioned artifacts."
            ]
          },
          "highlights": {
            "title": "Key Technical Highlights",
            "subtitle": "Engineering choices that shaped the solution.",
            "cards": [
              {
                "title": "Cloud-native feature pipeline",
                "body": "Athena + S3 enabled scalable feature assembly and reproducible dataset snapshots."
              },
              {
                "title": "Robust modeling approach",
                "body": "LightGBM chosen for strong performance on tabular data and resilience to common data issues."
              },
              {
                "title": "Stability-first validation",
                "body": "Evaluation across development, test, and OOT windows to reduce performance surprises in production."
              },
              {
                "title": "Explainability with SHAP",
                "body": "Global feature importance and driver analysis to support governance and stakeholder trust."
              }
            ]
          },
          "role": {
            "title": "Role & Responsibilities",
            "subtitle": "Ownership across data, modeling, evaluation, and stakeholder communication.",
            "items": [
              "Designed the end-to-end modeling workflow (data → features → training → validation → reporting).",
              "Built feature engineering and dataset assembly logic on AWS (Athena/S3) and Python on SageMaker.",
              "Trained and tuned a LightGBM classifier and validated performance on Train/Test and OOT periods.",
              "Produced ROC/Gini and other evaluation artifacts to define operating thresholds and trade-offs.",
              "Generated explainability outputs (SHAP summaries) to communicate model drivers.",
              "Prepared stakeholder-ready documentation and impact-test results for decision-making and adoption."
            ]
          },
          "stack": {
            "title": "Tech Stack",
            "subtitle": "Grouped by the layers used in the workflow.",
            "groups": [
              {
                "title": "Data & Storage",
                "items": [
                  "AWS S3",
                  "AWS Athena"
                ]
              },
              {
                "title": "Modeling & Compute",
                "items": [
                  "AWS SageMaker",
                  "Training Workers (Compute Instances)",
                  "Python"
                ]
              },
              {
                "title": "ML & Explainability",
                "items": [
                  "LightGBM",
                  "SHAP",
                  "Scikit-learn"
                ]
              }
            ]
          },
          "lessons": {
            "summary": "Lessons learned",
            "items": [
              "Out-of-time evaluation is essential for risk models to validate stability beyond random splits.",
              "Explainability accelerates stakeholder alignment and supports governance requirements.",
              "Clear impact testing bridges the gap between model metrics and business adoption decisions.",
              "Reproducible data snapshots (feature store-style exports) simplify iteration and auditability."
            ]
          }
        }
      }
    },
    {
      "slug": "anomaly-detection/data-quality-monitoring",
      "title": "Data Latency & Reporting Feasibility Analysis",
      "domain": "Mining / Data Quality",
      "problem": "Unclear data arrival timing across sources when defining reporting frequency.",
      "impact": "Enabled realistic reporting cadence and informed targeted improvements when needed.",
      "highlights": [
        "Assessed data latency across MongoDB, SQL Server alarm data, and GPS sources to define feasible near real-time reporting.",
        "Supported operational and commercial decisions by identifying optimal reporting windows per client.",
        "Enabled follow-up improvements and validated progress through repeated, on-demand analysis."
      ],
      "tags": [
        "Python",
        "MongoDB",
        "SQL Server",
        "Power BI",
        "Data Quality",
        "Latency Analysis"
      ],
      "i18n": {
        "dataQuality": {
          "hero": {
            "eyebrow": "Data Latency Assessment",
            "title": "Data Latency & Reporting Feasibility Analysis",
            "subtitle": "On-demand analysis to evaluate data arrival timing and define feasible reporting frequency across clients.",
            "chips": [
              "Python",
              "MongoDB",
              "SQL Server",
              "Power BI"
            ],
            "confidentialNote": "Screenshots and diagrams are anonymized due to NDA."
          },
          "panel": {
            "focus": {
              "label": "Focus",
              "value": "Data arrival latency and reporting feasibility"
            },
            "outcome": {
              "label": "Outcome",
              "value": "Defined optimal reporting cadence per client"
            },
            "audience": {
              "label": "Audience",
              "value": "Remote support teams and contract administrators"
            },
            "scope": {
              "label": "Deployment Scope",
              "countries": [
                "Peru",
                "Brazil",
                "Chile",
                "Colombia",
                "Mexico"
              ]
            }
          },
          "summary": {
            "title": "Project Summary",
            "subtitle": "Why and how reporting cadence was defined.",
            "cards": [
              {
                "title": "Problem",
                "body": "Reporting frequency needed to be aligned with actual data arrival behavior across systems."
              },
              {
                "title": "Solution",
                "body": "Performed on-demand latency analysis across MongoDB, complementary SQL Server alarm tables, and GPS data."
              },
              {
                "title": "Impact",
                "body": "Provided a clear reference for near real-time reporting and guidance on when improvements were required."
              },
              {
                "title": "Users",
                "body": "Remote support engineers and contract administrators responsible for operational monitoring."
              }
            ]
          },
          "dashboard": {
            "title": "Key Dashboard Views",
            "subtitle": "Visual tools used to assess reporting feasibility.",
            "views": [
              {
                "alt": "Power BI overview showing latency distribution by data source with color-coded criticality",
                "title": "Latency Overview by Source",
                "bullets": [
                  "Aggregated latency by data source to quickly understand typical arrival times.",
                  "Color scale indicating acceptable, moderate, and high delay ranges.",
                  "Used to define a realistic reporting window without manual calculations."
                ]
              },
              {
                "alt": "Power BI detail view showing per-vehicle average daily latency",
                "title": "Per-Asset Latency Detail",
                "bullets": [
                  "Drill-down to vehicle-level latency for deeper operational insight.",
                  "Helps distinguish systemic behavior from isolated cases.",
                  "Useful for validating improvements after network or process changes."
                ]
              }
            ]
          },
          "architecture": {
            "title": "Data Analysis Architecture",
            "subtitle": "Sources and processing used for latency assessment.",
            "alt": "Architecture diagram for latency analysis using MongoDB, SQL Server, GPS data, Python processing, and Power BI",
            "items": [
              "Data sources included MongoDB, SQL Server alarm data (complementary tables), and GPS records.",
              "Processing in Python and SQL Server for calculating latency metrics.",
              "Results exported as CSV files for flexible, client-specific analysis.",
              "Power BI dashboards loaded directly from CSV folders for rapid updates."
            ]
          },
          "highlights": {
            "title": "Key Technical Highlights",
            "subtitle": "What made the analysis practical and reusable.",
            "cards": [
              {
                "title": "On-demand assessment",
                "body": "Designed to be executed once or repeated as needed, rather than continuous monitoring."
              },
              {
                "title": "Latency bucketing",
                "body": "Grouped delays into intuitive ranges (minutes to days) to support decision-making."
              },
              {
                "title": "Multi-source coverage",
                "body": "Considered MongoDB, SQL Server alarm data, and GPS sources for a complete view."
              },
              {
                "title": "Operational enablement",
                "body": "Dashboard visuals allowed non-technical users to interpret results quickly."
              }
            ]
          },
          "role": {
            "title": "Role & Responsibilities",
            "subtitle": "Collaboration across analysis, tooling, and enablement.",
            "items": [
              "Defined the methodology to assess data latency and reporting feasibility.",
              "Collaborated with remote support teams to extract and validate data.",
              "Implemented Python processing and Power BI dashboards.",
              "Trained remote support personnel to perform future analyses independently.",
              "Supported contract administrators with data-driven reporting guidance."
            ]
          },
          "stack": {
            "title": "Tech Stack",
            "subtitle": "Tools used across the workflow.",
            "groups": [
              {
                "title": "Data Sources",
                "items": [
                  "MongoDB",
                  "SQL Server",
                  "GPS Data"
                ]
              },
              {
                "title": "Processing",
                "items": [
                  "Python",
                  "CSV"
                ]
              },
              {
                "title": "Visualization",
                "items": [
                  "Power BI"
                ]
              }
            ]
          },
          "lessons": {
            "summary": "Lessons learned",
            "items": [
              "Reporting frequency should reflect observed data arrival, not assumptions.",
              "A single, well-executed analysis is often sufficient to define reporting cadence.",
              "Clear visuals help align technical findings with operational and contractual decisions.",
              "Knowledge transfer ensures sustainability beyond the initial analysis."
            ]
          }
        }
      }
    },
    {
      "slug": "anomaly-detection/sensor-failure-gps",
      "title": "Reverse Sensor Failure Detection",
      "domain": "Mining / Telematics",
      "problem": "Detect reverse sensor failures before downtime.",
      "impact": "90% precision, enabled proactive maintenance.",
      "highlights": [
        "Implemented an algorithm to detect reverse sensor failures with 90% accuracy using GPS data and kinematic models."
      ],
      "tags": [
        "Python",
        "Pandas",
        "SQL"
      ]
    },
    {
      "slug": "graph-ml/mine-sectorization",
      "title": "Mine Sectorization with Graph Clustering",
      "domain": "Mining / Graph ML",
      "problem": "Sectorize the mine into strategic monitoring areas.",
      "impact": "Structured monitoring zones using clustering and label propagation.",
      "highlights": [
        "Developed a model to sectorize the mine into strategic areas using clustering, graph theory, and label propagation."
      ],
      "tags": [
        "Python",
        "Graph Theory",
        "Clustering"
      ]
    },
    {
      "slug": "dashboards/operator-behavior-monitor",
      "title": "Operator Alert Monitoring and Driving Score",
      "domain": "Safety / Operations",
      "problem": "Score operator driving to take preventive action.",
      "impact": "Preventive measures guided by operator alert scoring.",
      "highlights": [
        "Built an operator alert monitoring tool that scores driving to support preventive measures."
      ],
      "tags": [
        "Python",
        "Dashboards",
        "Safety"
      ]
    },
    {
      "slug": "dashboards/network-signal-kpi",
      "title": "Network Signal Quality Monitoring",
      "domain": "Network / Monitoring",
      "problem": "Monitor KPIs for field coverage and signal quality.",
      "impact": "Automated ETL and dashboards for network KPIs.",
      "highlights": [
        "Developed an automated ETL and KPI dashboard to monitor field coverage and signal quality."
      ],
      "tags": [
        "Python",
        "ETL",
        "Dashboards"
      ]
    },
    {
      "slug": "graph-ml/income-inference-graph",
      "title": "Income Inference with Graph Segmentation",
      "domain": "Banking / Graph ML",
      "problem": "Improve income inference accuracy with graph segmentation.",
      "impact": "+2% accuracy gain on income inference.",
      "highlights": [
        "Complemented the income inference model with a graph-based segmenter, improving accuracy by +2%."
      ],
      "tags": [
        "Python",
        "Graph Theory",
        "ML"
      ]
    },
    {
      "slug": "ml-systems/nrt-alerting-pipeline",
      "title": "ML Controls Monitoring Reports",
      "domain": "Banking / Model Ops",
      "problem": "Detect incidents in monthly ML control execution.",
      "impact": "20+ summarized reports to catch stability and score issues.",
      "highlights": [
        "Delivered 20+ summarized reports to detect incidents in monthly execution controls for retail risk and collections ML models."
      ],
      "tags": [
        "Python",
        "Reporting",
        "Model Monitoring"
      ]
    }
  ]
}